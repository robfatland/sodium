{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b0096b-3571-4ac2-95e2-b0237a4fd065",
   "metadata": {},
   "source": [
    "[This Notebook on the web](https://github.com/robfatland/othermathclub/blob/master/neuralnetworks.ipynb)\n",
    "\n",
    "\n",
    "[Questions](#Questions)\n",
    "\n",
    "\n",
    "# Understanding neural networks and artificial intelligence models\n",
    "\n",
    "\n",
    "This would naturally be broken up into multiple `.ipynb` files at some point.\n",
    "\n",
    "\n",
    "## Part 1 Reference Resource and Redux\n",
    "\n",
    "### Learning resources: Understanding neural networks and AI models today\n",
    "\n",
    "- [Grant Sanderson's Neural Network series at 3Blue1Brown (YouTube)](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "    - A series of 7+ \"20 minute\" videos; unsurpassed presentation style; requires frequent \"pause and ponder\" not to mention rewind/re-watch\n",
    "- [Steve Brunton's Machine Learning Primer](https://www.youtube.com/watch?v=Vx2DpMgplEM)\n",
    "- [Michael Nielsen's book on Deep Learning and Neural Nets](http://neuralnetworksanddeeplearning.com/)\n",
    "    - [Chapter 1 in particular gets right to it](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "    - [M. Nielsen's repo for the MNIST OCR walkthrough](https://github.com/mnielsen/neural-networks-and-deep-learning)\n",
    "- [Gil Strang's course reference at MIT](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/)\n",
    "    - [Gil Strang: First in a CNN lecture series](https://www.youtube.com/watch?v=sx00s7nYmRM&pp=ygUaZ2lsIHN0cmFuZyBuZXVyYWwgbmV0d29ya3M%3D)\n",
    "- [Chris Olah's blog](http://colah.github.io/)\n",
    "- [Andrej Karpathy: Build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=0s)\n",
    "- [Articles at *distill*](https://distill.pub/)\n",
    "- [Samson Zhang: Build an MNIST neural net from scratch (NumPy not PyTorch)](https://www.youtube.com/watch?v=w8yWXqWQYmU)\n",
    "    - Avoids higher abstractions like PyTorch\n",
    "\n",
    "\n",
    "### Looking back: How did we get here? (History) \n",
    "\n",
    "\n",
    "- [Historical background on machine learning / neural nets](https://www.youtube.com/watch?v=1il-s4mgNdI&t=0s)\n",
    "- [AlexNet as an important historical milestone](https://www.youtube.com/watch?v=UZDiGooFs54)\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "\n",
    "#### Particular to image classification CNNs\n",
    "\n",
    "\n",
    "- [ImageNet](https://en.wikipedia.org/wiki/ImageNet) > [Download site](https://www.image-net.org/download.php)\n",
    "- [CIFAR-10/100](https://en.wikipedia.org/wiki/CIFAR-10) > [Download site](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [MNIST](https://en.wikipedia.org/wiki/MNIST_database) > [Download site](https://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "\n",
    "\n",
    "### Fundamentals\n",
    "\n",
    "\n",
    "**Machine Learning** uses data to somehow determine how a model behaves.\n",
    "\n",
    "\n",
    "The *lesson* from the past couple decades: Scale alone gives huge improvement\n",
    "in model behavior.\n",
    "\n",
    "\n",
    "Model development involves a *training* phase where parameters (primarily weights\n",
    "used in weighted sums) are established. When the model is ready for use: We want to\n",
    "produce a response to an input. This process is *inferrence*. \n",
    "\n",
    "\n",
    "SGD makes use of first derivative only (in the Taylor series sense) and takes\n",
    "comparatively many small steps. By virtue of the *Stochastic*: it is not a precise\n",
    "but rather is more of a drunkard's walk, this being an attempt to speed things\n",
    "up even further. By comparison using Newton's method requires both the Jacobian\n",
    "(first derivative) and Hessian (second derivative) matrices and consequently takes\n",
    "larger and more precise steps towards a minimum of the cost function. However there\n",
    "is also the risk of starting out in the wrong place and winding up on a run away\n",
    "to infinity track.\n",
    "\n",
    "\n",
    "Contrast **FNN** with **RNN**: Feedforward neural networks only move information in \n",
    "the forward direction. They are trained using the (incongruously named) backpropagation\n",
    "method. Recurrent neural networks, in contrast, feature information loops or what we\n",
    "could term time-dependent feedback mechanisms, cf LSTM.\n",
    "\n",
    "\n",
    "How parameters (and hyper?) factor into the game...\n",
    "\n",
    "\n",
    "- Learning rate $s$ moderates step size in gradient descent (but I think not for Newton?)\n",
    "- Bias $b$ raises the bar of activation. You have to really *want* it\n",
    "- Softmax temperature $T$ tends to drive up probabilities relative to most likely\n",
    "    - Look for cases where an API might arbitrarily `ceil` $T$ so the LLM avoids looking bad\n",
    "\n",
    "\n",
    "Attention heads (93 in the case study) plus *feedforward* multi-layer perceptrons (MLPs) comprise one\n",
    "iteration of a transformer. \n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "\n",
    "Definition: AI is Artificial Intelligence writ large, includes ML and so on. gen-AI is somewhere\n",
    "in the AI subset space; and the distinction is usually obvious by context but let's take care not\n",
    "to *mean* generative but make it possible to infer broad AI, and vice versa. Like it or not we are\n",
    "going to have confusion.\n",
    "\n",
    "\n",
    "Input from the na√Øf... and I will add some fragmentary 'response thinking' in **bold font**.\n",
    "\n",
    "\n",
    "- Help me understand the dimensionality of the space that AI and particularly models address\n",
    "    - Tell me about the \"open-ness\" axis of this space\n",
    "        - What is awesomely cool about OLMo? Is that a fair premise?\n",
    "    - Tell me about the size of various models\n",
    "        - How to evaluate \"this will run on my laptop\" vs \"needs a big cloud server\" etcetera\n",
    "        - How do I understand the scale of inference in relation to training and parameter count?\n",
    "    - Tell me about pipelines. What is a pipeline? Why do I need a pipeline?\n",
    "        - Tell me why I need to understand `langchain`. What is it? What does it do?\n",
    "    - Tell me about hyperparameters: Again what do I need to know? What is beyond my concern?\n",
    "        - Learning phase versus Inference phase\n",
    "        - Temperature\n",
    "        - Learning rate\n",
    "        - Bias\n",
    "    - I need to create a model and somebody told me to use PyTorch\n",
    "        - Should I? I mean... what is it? what does it do for me?\n",
    "        - Is PyTorch a slow abstraction compared to something like Cuda?\n",
    "- Help me understand tokenization and embedding\n",
    "    - ...is tokenizing mysterious and arcane? Or easy to implement (and how?)\n",
    "    - ...is there something useful I can do in just the embedding space?\n",
    "        - i.e. without resorting to all this chat bot nonsense?\n",
    "- Same thing for Adversarial Networks (GAN too)\n",
    "- I have a very specific language-based task in mind...\n",
    "    - i.e not desultory 'chat chat chat'... but LLM-centric, gen-AI applied to a very focused task\n",
    "    - RAG or fine tune maybe? What are the tradeoffs? Which do I try first?\n",
    "    - Are there other accessible approaches to specializing?\n",
    "- What is this \"Foundation Model\" concept promoted by AWS?\n",
    "- Help me understand prompt engineering...\n",
    "   - specifically different approaches to prompting to get to what I want\n",
    "   - ...I want to know what \"Zero shot\" means, and jargon like that\n",
    "- Why is there so much hype around gen-AI...\n",
    "    - ...when I can easily stump Open AI: It can't perform a trivial task\n",
    "        - ...for example \"Write a version of Goldilocks in the style of Ladle Rat Rotten Hut\"\n",
    "- I understand containerization (a bit). Is it an important tool in AI applications?\n",
    "    - If so: What sort of learning journey do I need to go on to make use of it?\n",
    "- What is a diffusion model? Or *stable* diffusion?\n",
    "    - **thermodynamics-inspired image generation arXiv:2209.04747**\n",
    "    - unconditioned versus conditioned diffusion models: distinction?\n",
    "    - (wringing hands) At what point is the jargon useful?\n",
    "    - Same for transformers... there seems to be both the concept and the library...\n",
    "    - Same for attention heads, stochastic gradient descent, ...etcetera...\n",
    "    - The underlying question here: Is there a strategy for coping with the profusion of jargon?\n",
    "- Research focused \"How do I get started using AI methods to my data?\"\n",
    "    - Suppose I am using gen-AI to write code...\n",
    "        - Can I also get it to compile and run the code and check the results?\n",
    "    - Suppose I have data that can be stored in tabular form, say in CSV files\n",
    "        - How do I get started on applying AI methods to this data?\n",
    "\n",
    "\n",
    "## Part 2: 3Blue1Brown Video Series\n",
    "\n",
    "\n",
    "Probably as good or better than any other place to start: Grant Sanderson's series\n",
    "explains neural networks and, in later videos, the basic plan of large language\n",
    "models. He works specifically from the example of Chat-GPT 3 and includes a\n",
    "careful accounting of how this LLM came to have 170 billion parameters.\n",
    "\n",
    "\n",
    "These notes are in a *student format*: Interpreting the narrative in my own\n",
    "words.\n",
    "\n",
    "\n",
    "### 3Blue1Brown Video 1: But what is a neural network?\n",
    "\n",
    "\n",
    "We have a number of digital greyscale image, 28 x 28 pixels, each pixel \n",
    "having a value on [0, 1]; and the assumption is that each picture is \n",
    "of precisely one base-10 digit { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }. \n",
    "\n",
    "\n",
    "- We will conceptually build, train and test a neural network to interpret these images\n",
    "    - We have, somewhere, the \"correct answer\" recorded\n",
    "- The 28 x 28 pixel grid gives 784 neurons\n",
    "    - Pixel/neuron values have an *activation* level, a real value on [0, 1]\n",
    "    - The linear stacking of these pixels is maybe called the *activation layer*\n",
    "        - Don't care how the pixels are ordered in this layer\n",
    "        - That is: The list (or vertical stacking) of pixels does not retain the spatial information of the grid\n",
    "- A second layer has 16 neurons\n",
    "    - Connecting the source information to the second layer requires 784 x 16 weights\n",
    "- Third layer: 16 more neurons\n",
    "- Fourth layer: 10 'digit choice' output neurons\n",
    "    - For a given picture input we want all of these neurons at zero except the correct answer, at 1\n",
    "- There are also 16 bias values for the second layer: One for each layer-2 neuron\n",
    "    - Bias acts like a threshold: It sets a bar to clear\n",
    "    - Und so weiter biases for the remaining layers\n",
    "    - There is no bias for layer 1\n",
    "- The result of weighted sum plus bias is unconstrained...\n",
    "    - ...but we want a value on (say) [0, 1]\n",
    "    - Analogous to a compressor in audio signal processing\n",
    "    - Compression was originally done with a sigmoid function\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "More recently compression is via a Rectifier Linear Unit function: $ReLU(a) = max(0, a)$.\n",
    "Pronounced \"Ray - Loo\". I will continue to indicate compression as $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff469b-a5b9-409c-99ed-338969d94a33",
   "metadata": {},
   "source": [
    "- sum of weights = 784 * 16 + 16 * 16 + 16 * 10\n",
    "- sum of biases = 16 + 16 + 10\n",
    "\n",
    "\n",
    "13k total parameters\n",
    "\n",
    "\n",
    "Exhortation: Dig into ***why?*** Challenge your assumptions.\n",
    "\n",
    "\n",
    "- Activations are a $1 \\times n$ column vector $A$\n",
    "- Weights are an $m \\times n$ matrix $W$\n",
    "- Bias is a $1 \\times m$ column vector\n",
    "- Second layer of neurons is a $1 \\times m$ column vector $N$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\n",
    "N = \\sigma( W \\cdot A + B )\n",
    "}\n",
    "\\end{align}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7909d52-d14c-47f7-843c-3898b0fc0561",
   "metadata": {},
   "source": [
    "### Video 2: Gradient descent and how neural networks learn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Introduces a training dataset: With answers!\n",
    "- MNIST database is freely available\n",
    "- Now we start in on *the calculus exercise*\n",
    "- Initial: Random values\n",
    "\n",
    "\n",
    "Introduce a cost function: \n",
    "\n",
    "\n",
    "- Go through the (presumably wrong) values in the output vector $a_i$.\n",
    "- For each: Difference the value from the correct value $0, 0, 0, 0, 1, 0, 0, 0, 0, 0$\n",
    "- Square this\n",
    "- Add them up\n",
    "- Higher cost value: The worse this set of weights performed\n",
    "- This is a result for but one example\n",
    "    - This produces a single number from the combination of 784 pixel values and 13k weights and biases\n",
    "\n",
    "\n",
    "Consider the average cost over the entire training set.\n",
    "\n",
    "\n",
    "Now drop into single-variable calculus thinking: Use the local slope to determine a step to take\n",
    "in searching for a minimum. Steeper slope: Bigger step. Small slope: Small step (avoid overshoot).\n",
    "\n",
    "\n",
    "Now move to cost as a surface above two variables (up from one). Now we are taking gradients.\n",
    "\n",
    "\n",
    "What *moves* as we calculate the local surface gradient? Those two variables represent a\n",
    "simplification of the idea from above of many hundreds / thousands of weights. Suppose they are\n",
    "the first two weights $w_1$ and $w_2$. The cost function is a surface generated by the two-D\n",
    "space of weights 1 and 2. For a particular value of $w_1$ and $w_2$ we have a direction of\n",
    "steepest ascent (the gradient) and we use its negative to get a descent vector. We can follow\n",
    "this down to arrive at some local minimum.\n",
    "\n",
    "\n",
    "And now of course go from 2 dimensions to 13,002 dimensions of input.\n",
    "\n",
    "\n",
    "$- \\nabla C(\\vec{w})$\n",
    "\n",
    "\n",
    "Gradient calculation in this context is called *back propagation*: Next video.\n",
    "\n",
    "\n",
    "\n",
    "All we mean when we say a network is learning is that it is minimizing its cost function. \n",
    "\n",
    "\n",
    "\n",
    "- We have multiple layers of neurons, weights, biases to start with\n",
    "- These represent a progression from 784 inputs to one result\n",
    "    - the result is the selection of one grandmother neuron for one of { 0, 1, 2, ..., 9 }\n",
    "- We have test data including *correct* answers\n",
    "- We can determine cost function values for the entire input test dataset\n",
    "- This cost function $C$ has a mean value\n",
    "- Somehow we can calculate the gradient of $C$: A 13,000 element vector\n",
    "    - ...whose elements indicate changes to apply; by both sign and magnitude\n",
    "    - ...so we can modify the weights into new weights\n",
    "    - ...and iterate\n",
    "\n",
    "\n",
    "Then there is **the test**: Score a version of the network on data it has never seen before.\n",
    "\n",
    "\n",
    "\n",
    "### Does the network behave understandably?\n",
    "\n",
    "\n",
    "\n",
    "- No. \n",
    "- The weights from the 784 layer to the 16 layer (when viewed as 28 x 28 images) are just random-looking...\n",
    "- ...all 16 of them...\n",
    "- ...and will classify random junk as a particular digit with high confidence\n",
    "- ...and has no mechanism to actually *draw* an archetype 3\n",
    "\n",
    "\n",
    "Grant observes that there is no mechanism in the system for uncertainty. The result vectors\n",
    "for example are always certain: 9 zeros and a single 1.\n",
    "\n",
    "\n",
    "The video concludes with remarks on \"memorizing the dataset\" by means of all these parameters; \n",
    "what I suspect is called over-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be14871-1fd6-4f33-a3de-c3a2d4159daf",
   "metadata": {},
   "source": [
    "### Video 3: Backpropagation: Plausible story\n",
    "\n",
    "\n",
    "- Backpropagation as a compelling story without notation or calculus\n",
    "- Consider weights and activations in the penultimate layer\n",
    "    - Proceeding from the weighted sum: Converse concept:\n",
    "        - \"Modify Weights where Activations are high for impact!\"\n",
    "        - \"Modify Activations where Weights are high for impact!\"\n",
    "    - Complexity concept:\n",
    "        - We are considering 10 (not just the correct +1 neuron) end activations\n",
    "        - Multiplexing means individual cases are given a vote, not final say\n",
    "\n",
    "\n",
    "We want to find a minimum, hence gradient descent. \n",
    "\n",
    "\n",
    "### Video 4: Backpropagation: Calculus basis\n",
    "\n",
    "\n",
    "\"What is SGD?\"\n",
    "\n",
    "\n",
    "- Backpropagation as calculus\n",
    "- Principle idea is multi-variate calculus chain rule\n",
    "- A given end-neuron (e.g. answer = 3) is impacted by an activation (from the prior neuron layer), a weight and a bias\n",
    "- The idea is to get the gradient of the cost function in terms of chain rule partial derivatives\n",
    "- Back-prop refers to chaining backwards up the network until you arrive at the stimuls activation layer. These can not be modified.\n",
    "- Also we can think in terms of modifying other feeder neuron activation layers...\n",
    "    - ...but this is actually expressed in terms of -- in turn -- that neuron's inbound weight and bias\n",
    "- The other dial here\n",
    "    - We calculate the cost function gradient for one input case\n",
    "    - ...but in fact we do some sort of average over many training inputs\n",
    "    - ...but not every single one as this becomes computationally prohibitive\n",
    "    - ...so we resort to random samples creating cohorts\n",
    "    - ...hence stochastic\n",
    "    - ...hence stochastic gradient descent\n",
    "    - ...hence SGD\n",
    " \n",
    "\n",
    "> Note: When you use backpropagation you are in the Deep Learning space.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124c4b-c06f-4d20-b093-0e0a21b1928a",
   "metadata": {},
   "source": [
    "### Video 5: GPT and Transformers\n",
    "\n",
    "\n",
    "It is very important to call out the fact that our narrator makes an almost invisible\n",
    "transition here from talking about *Deep Neural Nets* (more than one hiddent layer)\n",
    "to Large Language Models (which build on DNNs). This can be a bit disorienting if we\n",
    "just walked in off the street. Furthermore the manner in which \"LMM is a kind of DNN\"\n",
    "never really gets spelled out; so we may need to seek further afield for this type of\n",
    "insight.\n",
    "\n",
    "\n",
    "* Transformers are the next key idea\n",
    "    * Consist of alternating **Attention Blocks** and **Perceptron Blocks**\n",
    "    * Predict the next word by generating a pdf over a set of words\n",
    "* First we need encoding as tokens (words or fragments)\n",
    "* Then we need embedding: Each token is assigned a vector value\n",
    "    * The vector space is \"defined\" by the model\n",
    "    * Similar words (I use words instead of tokens as roughly equivalent)...\n",
    "        * ...wind up with roughly aligned vector values\n",
    "    * The processing step is now to iterate through Attention and Perceptron blocks\n",
    "        * The Attention block allows the series of vectors to interact with one another\n",
    "            * ...hence the vector elements are modified\n",
    "        * The Perceptron blocks operate on the vectors in parallel\n",
    "            * ...so no interactivity in this step\n",
    "            * \"Multi-layer Perceptron\" (MLP) or equivalently \"Feed Forward Layer\"\n",
    "            * What this does is, for now, a mystery\n",
    "                * ...but it is certainly more linear algebra\n",
    "        * There is also some normalization going on, in passing\n",
    "     \n",
    "The end result is understood as the now-thoroughly-modified last vector in\n",
    "the token sequence. And this modified vector is then used to generate the pdf,\n",
    "a distribution of probabilities for a set of possible \"next words\".\n",
    "\n",
    "\n",
    "Now so far this is \"next word prediction\". To go to Chatbot we have\n",
    "\n",
    "- A System prompt (\"The Chatbot is a helpful yadda yadda\")\n",
    "- An initial User prompt\n",
    "\n",
    "Now in some sense the model is \"Predicting what that helpful assistant would say...\",\n",
    "an odd bit of rhetorical indirection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c86ba6-6584-407b-8b73-484e727adbb7",
   "metadata": {},
   "source": [
    "The model uses (100B+) weights organized into matrices, in turn \n",
    "divided up into eight categories.\n",
    "\n",
    "\n",
    "Pre-processing: Tokenization\n",
    "\n",
    "Categories\n",
    "\n",
    "\n",
    "- embedding: One entry for each token/word\n",
    "    - This translates our English query into a sequence of vectors\n",
    "    - GPT 3 uses 12,288 dimensions for these vectors\n",
    "    - Vector alignment (embedding similarity): Inner (dot) product\n",
    "    - 600 million weights in total just for token > embedded vector\n",
    "    - How many tokens are allowed?\n",
    "        - GPT 3: 2000 tokens, the **Context Size**\n",
    "    - Look-up (embedding) matrix is $W_e$\n",
    "- Together Query, Key and Value comprise one **Head** of attention\n",
    "    - query\n",
    "        - Lower-dimensional space: Asks about word modification of meaning\n",
    "    - key\n",
    "        - Lower-dimensional space: Answers query for relevance\n",
    "    - value\n",
    "        - Generates a vector based on a relevant modifier\n",
    "        - This is added to the embedding of the modified word\n",
    "        - ...and this changes its location in the embedding space\n",
    "    - Words that follow are not allowed to modify words that precede\n",
    "- output\n",
    "- up-projection\n",
    "- down-projection\n",
    "- unembedding\n",
    "    - Simple idea: Works on the very last highly-modified embedding vector\n",
    "        - So we will be looking at $12k$ values in just that last vector\n",
    "    - Reality is more complicated\n",
    "    - Call the un-embedding matrix $W_u$\n",
    " \n",
    "**Softmax** is an operation on a vector of arbitrary values mapping this to a \n",
    "pdf with sum 1 and values on [0, 1]. This gets us to the idea of *Temperature* \n",
    "which in this context is a user-chosen value that acts as a \"creativity dial\". \n",
    "Higher temperature means the GPT responses will stretch further down into the \n",
    "pdf (lower probabilities) to select that next word. \n",
    "\n",
    "If the vector $\\vec{x} = \\{ x_0, \\dots , x_{N-1} \\}$ then the softmax-modified\n",
    "value of $x_n$ (given a temperature $T$) is given by:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\\Huge {x'}_{n} = {e^{\\frac{x_n}{T}}}/{\\sum_{i=0}^{N-1}{e^{\\frac{x_i}{T}}}}\n",
    "}\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "Now we have an idea of softmax, of embedding, general use of \n",
    "matrix $\\times$ vector in this process, and inner product for similarity...\n",
    "so we're invited to the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee15be0-bc64-4fd9-96ec-adbd4fa9f714",
   "metadata": {},
   "source": [
    "### Video 6: Attention in Transformers\n",
    "\n",
    "\n",
    "\"Attention is all you need\" (2017)\n",
    "\n",
    "\n",
    "Embedding: ***Direction corresponds to semantic meaning***\n",
    "\n",
    "\n",
    "A mole is a mole is a mole (lookup table).\n",
    "\n",
    "\n",
    "Attention block \n",
    "* moves a given word's vector into a more accurate direction based on context words\n",
    "    * ...and those modifiers could be quite far away within the body of the input\n",
    "* consists of many *heads* running in parallel\n",
    "\n",
    "\n",
    "A hitherto skipped over detail: The embedding vector encodes the position of the token in\n",
    "the source: Location context. This in addition to the encoding of the word itself.\n",
    "\n",
    "#### Towards intuitive comprehension\n",
    "\n",
    "- The noun asks the question \"Am I being modified\" (a Query)\n",
    "    - and the adjective answers \"Yes by me!\" (a Key)\n",
    "    - The Query / Key space is a much lower dimensionality (e.g. 128)\n",
    "    - Query and Key are matrices (per head)\n",
    "        - ...that operate on the embeddings to reflect query and key being aligned\n",
    "        - the verb is that the modifiers *attend* to the target word / embedding\n",
    "\n",
    "- Softmax applied to the results gives a grid called an Attention Pattern\n",
    "- In Key Query space we tend to divide by the square root of the dimensionality\n",
    "- Acausal is avoided by setting post-token weights to zero\n",
    "    - Later tokens are not permitted to modify earlier ones \n",
    "    - This is actually done by setting the Attention Pattern element to $-\\inf$ prior to softmax\n",
    "    - This is called masking\n",
    "- Attention Pattern size is the square of the Context size\n",
    "    - So Context is a bottleneck\n",
    "    - Here are the names of approaches developed to address this\n",
    "        - Sparse Attention Mechanisms\n",
    "        - Blockwise Attention\n",
    "        - Linformer\n",
    "        - Reformer\n",
    "        - Ring attenuation\n",
    "        - Longformer\n",
    "        - Adaptive Attention Span\n",
    "     \n",
    "I am fairly certain that the Query Key > Attention Pattern is not modifying\n",
    "the embedding vectors just yet. Rather it is a setup for the next step (Value).\n",
    "That is: The Query / Key step produces an Attention Pattern matrix which will\n",
    "act as weights. \n",
    "\n",
    "\n",
    "...this is a little hard to grok from Grant's presentation... \n",
    "\n",
    "\n",
    "Grant is saying that we have not yet actually used the adjective embedding to \n",
    "modify the noun embedding; and so we turn now to a Value matrix which is left-multiplied\n",
    "to the adjective's embedding, result = vector, and this vector is then (I think?)\n",
    "multiplied by the Attention Pattern to give a $\\Delta$ modification to be added to the\n",
    "noun's embedding. This is the Value step. Need a few more watchings.\n",
    "\n",
    "\n",
    "And now we have the Key, Query, and Value matrices driving a modification of \n",
    "the input token embeddings, voila the Attention Block. This is one head of attention; \n",
    "of which there are many. \n",
    "\n",
    "\n",
    "Efficiency note: Make the Value map not square 13k x 13k but rather matching the Key Query\n",
    "dimensionality. There is an explanation on factoring the Value matrix into two matrices\n",
    "that are 12k x 128. \n",
    "\n",
    "\n",
    "Attention heads are of type 'self-attention head', contrast with 'cross-attention head' \n",
    "which process two distinct types of data. For example: Audio input of speech and transcription.\n",
    "So a translator system might have Query in one language and Key in the other.\n",
    "\n",
    "\n",
    "GPT-3 uses 96 attention heads in one attention block. Factor of 100 in the parameter count.\n",
    "\n",
    " \n",
    "Each head gives a modification $\\Delta$ for a given token embedding. All of them are applied.\n",
    "\n",
    "\n",
    "Finally: We are up in the 50B parameters... eventually 170B... so parallel processing (GPU access)\n",
    "is key. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eda18d-71d5-4319-b714-dbfc2c701269",
   "metadata": {},
   "source": [
    "### Video 7: How might LLMs store facts... deep learning\n",
    "\n",
    "\n",
    "Multi-Layer Perceptron (MLP): Maybe has something to do with facts. \n",
    "\n",
    "\n",
    "The Grandmother Neuron is abandoned in favor of information distributed as a pattern \n",
    "across many neurons. Look at the search term **sparse autoencoders**. Anthropic has \n",
    "articles with titles 'Toy Models of Superposition' and 'Towards Monosemanticity:\n",
    "Decomposing Language Models With Dictionary Learning'. \n",
    "\n",
    "\n",
    "\n",
    "Details not discussed in this series\n",
    "\n",
    "\n",
    "- Tokenization\n",
    "- Positional encoding\n",
    "- Layer normalization\n",
    "- Training (chapter pending) which is all about back-propagation\n",
    "    - Includes fine tuning\n",
    "\n",
    "\n",
    "A lot of this is reminiscent of Fourier transforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5ff7c-7b3c-4cb1-9406-3aa4b9aa33f6",
   "metadata": {},
   "source": [
    "## [Part 2 Michael Nielsen's Book, Chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "\n",
    "\n",
    "* Perceptron model is *binary output* $1$ or $0$ from evaluating $w_j \\cdot x_j + b \\ge 0$.\n",
    "* Perceptrons cover logic gates $NAND$ and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04510fd-6c3a-446d-bf04-3102c037f604",
   "metadata": {},
   "source": [
    "## Part 3 Gil Strang lectures\n",
    "\n",
    "\n",
    "### [Gradient Descent](https://www.youtube.com/watch?v=AeRwohPuUHQ)\n",
    "\n",
    "\n",
    "- Trying to minimize the cost (or any) multi-variable function...\n",
    "- \"When there are too many variables (weights) to take a second derivative we settle for first derivatives of the function\"\n",
    "- Let's introduce a learning rate parameter associated with dimension $k$, call this $s_k$\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "x_{k+1} = x_{k} - s_{k} \\; \\cdot \\; - \\nabla f(x_k)\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "Let's be on the lookout for the Hessian and the role played by convexity, spoken of in ominous tones by our lecturer.\n",
    "\n",
    "\n",
    "We have two independent variables $x$ and $y$ and a quadratic function *of* them:\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "f(x, y) = \\frac{1}{2} \\cdot (x^2 + by^2)\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "Let's write this in vector algebra form with a symmetric matrix **$S$** and these two variables \n",
    "organized as a column vector, pardon the redundant notation:\n",
    "\n",
    "\n",
    "$\\tilde{x}=\\left[{\\begin{array}{c}x\\\\y\\end{array}}\\right]$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "S=\\left[{\\begin{array}{cc}\n",
    "   1 & 0 \\\\\n",
    "   0 & b \\\\\n",
    "  \\end{array}}\\right]\n",
    "\\; \\; \\; \n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "And so now \n",
    "\n",
    "$\\begin{align}\n",
    "f(\\tilde{x}) = \\frac{1}{2} \\cdot \\tilde{x}^{T} \\cdot S \\cdot \\tilde{x}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae9316-6568-4f17-a9c9-86e3fbf01bee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c5fd9f-8bfb-4537-af39-74c8eadbf9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13b84e3-6988-46cf-a00e-a900143e1c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "x+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d84f74-9aad-44e3-8fb1-e9326682658a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d0b79-09eb-4561-a9a1-7c7016e38d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df3fc4-2ec5-4282-8985-cc4099b60d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb140e-32be-47e7-95e0-cbd20116b184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd006e-29f7-418d-a032-b90d32fc8d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028555d-b1c0-44e9-9635-3f77ca695671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
